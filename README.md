# Neural_network_project


Dataset:

### Experiment 1: Change activation function


* Relu


![Relu](https://github.com/smohammadi96/Neural_network_project/blob/main/images/relu.PNG)



* Tanh


![Tanh](https://github.com/smohammadi96/Neural_network_project/blob/main/images/tanh.PNG)



* Sigmoid


![Sigmoid](https://github.com/smohammadi96/Neural_network_project/blob/main/images/sigmoid.PNG)


* ELU


![ELU](https://github.com/smohammadi96/Neural_network_project/blob/main/images/ELU.PNG)


Activation function | Optimizer | Accuracy on test set | Loss | 
--- | --- | --- | --- |
Relu | 301 | 283 | 290 | 286 |
Tanh |  |||
ELU | |||
Sigmoid | |||
